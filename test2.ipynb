{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Georgia, serif;\">**Twitter Sentiment Analysis** :Understanding Emotions in Tweets about Apple and Google products.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](twits.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Problem**: Using Sentiment Analysis to Improve Apple and Google Product Marketing Strategies \n",
    "\n",
    "The introduction of social media has completely changed how businesses interact with their consumers and the general public in today's connected society. While the digital age offers limitless possibilities for marketing and brand development, it also brings its own set of difficulties. One of these difficulties is the inability of enterprises to precisely gauge public opinion and feelings towards their goods or services.\n",
    "\n",
    "In the age of social media, organizations are acutely aware of the need to harness the wealth of sentiment and emotion data available on these platforms. However, they often struggle to do so effectively, given the unprecedented speed, diversity, and complexity of social media communication. The dynamic nature of the medium, the diverse and contextual language used, the rapid increase of emojis and visual content, the volume of noise, and ethical concerns all contribute to the challenge of gauging public sentiment and emotions. \n",
    "\n",
    "To overcome these challenges, organizations must invest in advanced sentiment analysis tools and technologies, develop cultural and linguistic expertise, and strike a balance between data-driven insights and ethical considerations. By doing so, they can unlock the valuable insights hidden within the social media storm and use them to inform strategic decisions, enhance products and services, and build stronger connections with their audience in this rapidly evolving digital landscape.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Overview:**\n",
    "This dataset comprises 8,721 entries organized into three distinct columns: tweet_text, emotion_in_tweet_is_directed_at, and is_there_an_emotion_directed_at_a_brand_or_product. Each entry in the dataset represents a tweet along with associated metadata regarding the product or brand it's directed at and the emotional sentiment conveyed within the tweet.\n",
    "\n",
    "**Column Descriptions:**\n",
    "\n",
    "`tweet_text:`\n",
    "\n",
    "The tweet_text column contains the textual content of individual tweets. These tweets are typically concise, informal expressions shared by users on a social media platform, such as Twitter. Each tweet serves as a snapshot of a user's thoughts, opinions, or experiences related to a particular product or brand.\n",
    "The textual data within this column can vary in length, language, and complexity. It may include hashtags, mentions of other users, URLs, and a wide range of linguistic elements.\n",
    "Analysis of the tweet text can provide valuable insights into the sentiments, opinions, or feedback expressed by users regarding the product or brand.\n",
    "\n",
    "`emotion_in_tweet_is_directed_at:`\n",
    "\n",
    "The emotion_in_tweet_is_directed_at column provides information about the specific product or brand mentioned or targeted by each tweet. This column serves as a categorical label indicating the entity towards which the emotion or sentiment expressed in the tweet is directed.\n",
    "Entries in this column may include the names or identifiers of various products or brands, allowing for the categorization of tweets based on the entity they reference.\n",
    "Understanding which products or brands are most frequently mentioned in tweets can help identify consumer preferences and the areas where sentiment analysis may be most relevant.\n",
    "\n",
    "`is_there_an_emotion_directed_at_a_brand_or_product:`\n",
    "\n",
    "The is_there_an_emotion_directed_at_a_brand_or_product column characterizes the emotional sentiment or tone conveyed within each tweet directed at a product or brand.\n",
    "This column serves as a crucial indicator of the emotional context of the tweets and can be categorized into several classes, including:\n",
    "\n",
    "* Positive: Tweets expressing favorable sentiments, such as satisfaction, excitement, or endorsement, towards the product or brand.\n",
    "\n",
    "* Negative: Tweets containing unfavorable sentiments, such as criticism, frustration, or dissatisfaction, directed at the product or brand.\n",
    "\n",
    "* No Emotion: Tweets that do not convey any discernible emotional sentiment. These tweets may provide neutral or factual information.\n",
    "\n",
    "* Not Clear: Tweets where the emotional tone is ambiguous or unclear, making it challenging to determine the sentiment.\n",
    "\n",
    "Analyzing this column allows for sentiment classification and provides valuable insights into how consumers perceive and react to products or brands in the context of social media.\n",
    "Dataset Size:\n",
    "\n",
    "The dataset contains a total of 8,721 entries, each representing a unique tweet. This dataset size is substantial and provides a rich source of data for sentiment analysis and brand/product perception studies.\n",
    "\n",
    "**Data Exploration and Analysis:**\n",
    "\n",
    "To gain a deeper understanding of the dataset and its implications, exploratory data analysis (EDA) techniques, natural language processing (NLP) methods, and sentiment analysis tools can be applied.\n",
    "EDA involves a series of techniques and methods to gain insights into the structure, content, and patterns within the textual information. Here's a step-by-step guide on how to apply EDA to text data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading and Overview:** Here we begin by loading our text data into a preferred data analysis environment (e.g., Python with pandas).The data has non UTF-8 characters so we first create a function that removes the non UTF-8 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def remove_non_utf8(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "with open('data/judge_1377884607_tweet_product_company.csv', 'r', encoding='utf-8') as file:\n",
    "    cleaned_text = remove_non_utf8(file.read())\n",
    "\n",
    "df = pd.read_csv(io.StringIO(cleaned_text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the column 'is_there_an_emotion_directed_at_a_brand_or_product' to 'Sentiment'.\n",
    "\n",
    "column_name_mapping = {'is_there_an_emotion_directed_at_a_brand_or_product': 'Sentiment'}\n",
    "df.rename(columns=column_name_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling in missing (NaN) values with the string 'N/A' \n",
    "\n",
    "df['emotion_in_tweet_is_directed_at'].fillna('N/A', inplace=True)\n",
    "df['tweet_text'].fillna('N/A', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assign_brand(phrase):\n",
    "    \"\"\" \n",
    "    Takes in a phrase as input and returns\n",
    "    a brand label based on certain keywords found in the phrase.\n",
    "    \"\"\"\n",
    "    if 'iPad' in phrase or 'iPhone' in phrase :\n",
    "        return 'Apple'\n",
    "    elif 'Other Apple product or service' in phrase or 'Apple' in phrase:\n",
    "        return 'Apple' \n",
    "    elif 'iPad or iPhone App' in phrase:\n",
    "        return 'Apple'       \n",
    "    elif 'Google' in phrase or 'Other Google product or service' in phrase:\n",
    "        return 'Google'\n",
    "    elif 'Android App' in phrase or 'Android' in phrase:\n",
    "        return 'Android'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "\n",
    "#creating a new column called 'brand' that contains the assigned brand labels\n",
    "\n",
    "df['brand'] = df['emotion_in_tweet_is_directed_at'].apply(assign_brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing:Before conducting EDA, it's crucial to preprocess the text data. Common preprocessing steps include:\n",
    "* Lowercasing: Convert all text to lowercase to ensure consistency.\n",
    "* Tokenization: Split text into individual words or tokens.\n",
    "* Stop Word Removal: Eliminate common and uninformative words like \"the,\" \"and,\" \"in.\n",
    "* Punctuation Removal: Remove special characters, punctuation marks, and symbols.\n",
    "* Lemmatization or Stemming: Reduce words to their root form for better analysis.\n",
    "\n",
    "In the cell below we create a functions that integrates the above processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize,TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def clean_and_preprocess_text(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    # Remove mentions (words starting with '@') and URLs\n",
    "    tokens = [token for token in tokens if not token.startswith('@') and not token.startswith('http')]\n",
    "    # Remove punctuation and numbers using regular expressions\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Apply stemming using the Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    cleaned_text = ' '.join(stemmed_tokens) \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the function\n",
    "df['tweet_text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great stuff fri sxsw  marissa mayer  googl   tim oreilli  tech book  confer   matt mullenweg  wordpress '"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_and_preprocess_text(df['tweet_text'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column that contains processed text\n",
    "df['processed_text'] = df['tweet_text'].map(clean_and_preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to define a mapping which associates certain sentiment labels with numerical values.This is essential because NLP involve tasks such as regression that requires integer inputs.We can then use the numbers as reference.In the following mapping,'Positive emotion' is mapped to 1.0,'Negative emotion' is mapped to 0.0, 'No emotion toward brand or product' is mapped to 2.0 and 'I can't tell' is also mapped to 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>brand</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>Google's #geosocial Offers platform goes live ...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>googl geosoci offer platform goe live sxsw  link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>Still a big line outside of Apple's pop-up sho...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>still big line outsid appl popup shop   day ip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>omarg: It's not a rumor: Apple is opening up a...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>omarg  rumor  appl open temporari store downto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3280</th>\n",
       "      <td>Here is the video I took with my iPhone of @me...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>video took iphon super sxsw  link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4690</th>\n",
       "      <td>Andrew K of PRX equates the homogeneity of the...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>andrew k prx equat homogen appl ecosystem w pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "1627  Google's #geosocial Offers platform goes live ...   \n",
       "3461  Still a big line outside of Apple's pop-up sho...   \n",
       "1461  omarg: It's not a rumor: Apple is opening up a...   \n",
       "3280  Here is the video I took with my iPhone of @me...   \n",
       "4690  Andrew K of PRX equates the homogeneity of the...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  Sentiment brand  \\\n",
       "1627                             N/A        2.0   N/A   \n",
       "3461                             N/A        2.0   N/A   \n",
       "1461                             N/A        2.0   N/A   \n",
       "3280                             N/A        2.0   N/A   \n",
       "4690                             N/A        0.0   N/A   \n",
       "\n",
       "                                         processed_text  \n",
       "1627  googl geosoci offer platform goe live sxsw  link   \n",
       "3461  still big line outsid appl popup shop   day ip...  \n",
       "1461  omarg  rumor  appl open temporari store downto...  \n",
       "3280                 video took iphon super sxsw  link   \n",
       "4690  andrew k prx equat homogen appl ecosystem w pr...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a mapping dictionary\n",
    "sentiment_mapping = {'No emotion toward brand or product': 2.0,\n",
    "                  'Positive emotion': 1.0, \n",
    "                  'Negative emotion': 0.0,\n",
    "                  'I can\\'t tell': 2.0}\n",
    "\n",
    "# Use the .map() method to map values in column 'Sentiment' to new values\n",
    "df['Sentiment'] = df['Sentiment'].map(sentiment_mapping)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In subsequent stages of data preparation, we will construct a DataFrame that exclusively contains instances where the sentiment is either positive (1.0) or negative (0.0). This step is crucial for generating a dataset suitable for training our baseline model and assessing its performance when dealing with a binary target variable, specifically 0 or 1.When we get the data we split it into training and test for validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#creating new df where sentiment is either positive or negative\n",
    "bi_tar = df[(df['Sentiment'] == 0)| (df['Sentiment'] == 1)]\n",
    "\n",
    "X = bi_tar['processed_text']\n",
    "y = bi_tar['Sentiment']\n",
    "\n",
    "X_train_bi, X_test_bi, y_train_bi, y_test_bi = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating data for the iterated model that has multiple classes(targets)\n",
    "multi_tar = df.copy()\n",
    "X = multi_tar['processed_text']\n",
    "y = multi_tar['Sentiment']\n",
    "#One-hot encoding to ensure that each class label is treated as a separate, independent category. \n",
    "y_dummies = pd.get_dummies(y)\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X, y_dummies, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, Natural Language Processing (NLP) relies on regression techniques, necessitating the transformation of text data into numerical vectors that machine learning algorithms can comprehend and process effectively. This conversion can be achieved through various methods, such as GloVe (Global Vectors for Word Representation), Word2Vec, TF-IDF (Term Frequency-Inverse Document Frequency), and BERT (Bidirectional Encoder Representations from Transformers). In our particular scenario, we will opt for TF-IDF, which can be seamlessly incorporated into a function designed to convert our text data. This function will also incorporate Compressed Sparse Row (CSR) matrices to transform the TF-IDF training matrix into a space-efficient CSR matrix format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "def csr_Tfid_vect(X_train,X_test):\n",
    "    tf_idf_train = vectorizer.fit_transform(X_train)\n",
    "    tf_idf_test = vectorizer.transform(X_test)\n",
    "\n",
    "    tf_idf_train = csr_matrix(tf_idf_train)\n",
    "    tf_idf_test = csr_matrix(tf_idf_test)\n",
    "\n",
    "    return tf_idf_train,tf_idf_test\n",
    "\n",
    "X_tf_idf_train_bi,X_tf_idf_test_bi = csr_Tfid_vect(X_train_bi,X_test_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import text, sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the provided data and functions, we are prepared to embark on exploratory data analysis (EDA) and commence the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can calculate word frequencies, and print the top 10 most frequent words along with their normalized frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word       Normalized Frequency\n",
      "sxsw              0.0858       \n",
      "mention          0.06442       \n",
      "link             0.03821       \n",
      "rt               0.02743       \n",
      "ipad             0.02671       \n",
      "google            0.022        \n",
      "apple            0.01969       \n",
      "quot             0.01503       \n",
      "iphone           0.01414       \n",
      "store            0.01316       \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "\n",
    "# Concatenate all tweet text into a single string\n",
    "big_sentence = ' '.join(df['tweet_text'])\n",
    "\n",
    "# Tokenization pattern\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "\n",
    "# Tokenize and convert to lowercase\n",
    "tweets_raw = nltk.regexp_tokenize(big_sentence, pattern)\n",
    "tweets_raw = [word.lower() for word in tweets_raw]\n",
    "\n",
    "# Create a list of stopwords and add punctuation\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)\n",
    "\n",
    "# Remove stopwords and punctuation\n",
    "tweets_raw_stopped = [word for word in tweets_raw if word not in stopwords_list]\n",
    "\n",
    "# Calculate word frequency distribution\n",
    "tweets_freqdist = FreqDist(tweets_raw_stopped)\n",
    "\n",
    "# Calculate total word count\n",
    "total_word_count = sum(tweets_freqdist.values())\n",
    "\n",
    "# Get and print the top 10 most frequent words with normalized frequencies\n",
    "tweets_freqdist_top_10 = tweets_freqdist.most_common(10)\n",
    "print(f'{\"Word\":<10} {\"Normalized Frequency\":<20}')\n",
    "for word in tweets_freqdist_top_10:\n",
    "    normalized_frequency = word[1] / total_word_count\n",
    "    print(f'{word[0]:<10} {normalized_frequency:^20.4}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use NLTK to find and score the top bigram (two-word combinations) collocations in the tweets_raw_stopped text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('rt', 'mention'), 0.02666802617674867),\n",
       " (('sxsw', 'link'), 0.008527835968929014),\n",
       " (('link', 'sxsw'), 0.007656513598190615),\n",
       " (('sxsw', 'rt'), 0.006275374946701025),\n",
       " (('mention', 'mention'), 0.005728481118258838),\n",
       " (('mention', 'sxsw'), 0.005478207671344618),\n",
       " (('apple', 'store'), 0.005348436254426132),\n",
       " (('sxsw', 'mention'), 0.004755195491370201),\n",
       " (('link', 'rt'), 0.004718117943679205),\n",
       " (('mention', 'google'), 0.004356611853691997),\n",
       " (('social', 'network'), 0.0040970690198550265),\n",
       " (('new', 'social'), 0.003781909864481563),\n",
       " (('mention', 'rt'), 0.0031886691014256317),\n",
       " (('network', 'called'), 0.003021820136816151),\n",
       " (('store', 'sxsw'), 0.003021820136816151)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "tweets_finder = BigramCollocationFinder.from_words(tweets_raw_stopped)\n",
    "tweets_scored = tweets_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "tweets_scored[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SXSW is renowned for its conferences and festivals that commemorate the intersection of technology, film, music, education, and culture.\n",
    "RT is the inaugural 24/7 English-language news channel from Russia, offering a Russian perspective on worldwide news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to train a Word2Vec model on tokenized text data from the 'processed_text' column of our dataFrame. We can then retrieve the most similar words to specific words from the trained Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(708737, 978660)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "\n",
    "data = df['processed_text'].map(word_tokenize)\n",
    "model = Word2Vec(data, window=5, min_count=1, workers=4)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rt', 0.683062732219696),\n",
       " ('tnw', 0.6658665537834167),\n",
       " ('conveni', 0.6573855876922607),\n",
       " ('unit', 0.6483298540115356),\n",
       " ('ncbshow', 0.6436857581138611),\n",
       " ('cnt', 0.6349107623100281),\n",
       " ('pun', 0.63333660364151),\n",
       " ('steam', 0.6303820610046387),\n",
       " ('sink', 0.6300735473632812),\n",
       " ('motiv', 0.6265491843223572)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv = model.wv\n",
    "wv.most_similar('sxsw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cosbi', 0.4372282028198242),\n",
       " ('dpe', 0.23546390235424042),\n",
       " ('shortcut', 0.23334373533725739),\n",
       " ('linney', 0.22220098972320557),\n",
       " ('laura', 0.21703214943408966),\n",
       " ('hootstat', 0.19113896787166595),\n",
       " ('ure', 0.17929907143115997),\n",
       " ('pah', 0.17042851448059082),\n",
       " ('comedi', 0.1692882627248764),\n",
       " ('sketchi', 0.1489688605070114)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(negative='sxsw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Model**: Sentiment is either positive(1)or negative(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to set up and evaluate three machine learning models (Random Forest, Support Vector Machine, and Logistic Regression) using cross-validation.The output will provide us with an idea of how well each model is performing on our dataset based on the specified cross-validation scheme (in out case, 2-fold cross-validation). We can assess which model is the most suitable for our specific classification task based on these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    7.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.8637869451193023),\n",
       " ('Support Vector Machine', 0.8582943167130577),\n",
       " ('Logistic Regression', 0.8447452254919312)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([('Logistic Regression', LogisticRegression())])\n",
    "\n",
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]\n",
    "\n",
    "scores = [(name, cross_val_score(model,X_tf_idf_train_bi, y_train_bi, cv=2).mean()) for name, model, in models]\n",
    "scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these scores, the Random Forest model achieved the highest mean accuracy, followed by the Support Vector Machine and Logistic Regression models. We can use the Random Forest model as our final baseline model.\n",
    "\n",
    "Moving on we can implement a neural network for our classification task.We can use deep learning libraries such as TensorFlow or PyTorch.In our case we will use keras which was originally developed as a separate library but has been tightly integrated into TensorFlow.We will use a \"Sequential\" model which is a specific type of neural network architecture available within Keras. It's designed for building feedforward neural networks, where layers are stacked sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2731, 3978)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tf_idf_train_bi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "zty = X_tf_idf_train_bi.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 64)                254656    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256769 (1003.00 KB)\n",
      "Trainable params: 256769 (1003.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(Dense(units=64, input_shape=(3978,)))\n",
    "model_1.add(Dropout(0.5))\n",
    "\n",
    "model_1.add(Dense(32, activation='relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_1.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 7s 36ms/step - loss: 0.5071 - accuracy: 0.8291 - val_loss: 0.4136 - val_accuracy: 0.8358\n",
      "Epoch 2/5\n",
      "77/77 [==============================] - 2s 22ms/step - loss: 0.3841 - accuracy: 0.8437 - val_loss: 0.3718 - val_accuracy: 0.8358\n",
      "Epoch 3/5\n",
      "77/77 [==============================] - 2s 23ms/step - loss: 0.2919 - accuracy: 0.8571 - val_loss: 0.3333 - val_accuracy: 0.8504\n",
      "Epoch 4/5\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1926 - accuracy: 0.9316 - val_loss: 0.3090 - val_accuracy: 0.8577\n",
      "Epoch 5/5\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1142 - accuracy: 0.9703 - val_loss: 0.3388 - val_accuracy: 0.8650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1dbaaea15a0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(zty, y_train_bi, epochs=5, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterated Model**: Sentiment is either positive(1),negative(0),No emotion toward brand or product(2) or Not clear(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf_idf_train_multi ,X_tf_idf_test_multi = csr_Tfid_vect(X_train_multi,X_test_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6976, 6486)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tf_idf_train_multi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pty = X_tf_idf_train_multi.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "218/218 [==============================] - 10s 32ms/step - loss: 0.8727 - accuracy: 0.5992\n",
      "Epoch 2/5\n",
      "218/218 [==============================] - 7s 33ms/step - loss: 0.7239 - accuracy: 0.6829\n",
      "Epoch 3/5\n",
      "218/218 [==============================] - 7s 34ms/step - loss: 0.6150 - accuracy: 0.7456\n",
      "Epoch 4/5\n",
      "218/218 [==============================] - 7s 34ms/step - loss: 0.5233 - accuracy: 0.7901\n",
      "Epoch 5/5\n",
      "218/218 [==============================] - 7s 34ms/step - loss: 0.4564 - accuracy: 0.8212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1dbb6220c40>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "\n",
    "model_2.add(Dense(64, activation='relu', input_shape=(6486,)))\n",
    "model_2.add(Dropout(0.5))\n",
    "\n",
    "model_2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model_2.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_2.fit(pty, y_train_multi, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 1s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.002417  , 0.01918484, 0.9783982 ],\n",
       "       [0.0041768 , 0.09200642, 0.90381676],\n",
       "       [0.21975414, 0.32695657, 0.4532893 ],\n",
       "       ...,\n",
       "       [0.12181512, 0.4919104 , 0.3862745 ],\n",
       "       [0.02306609, 0.0343857 , 0.9425483 ],\n",
       "       [0.02364898, 0.6384658 , 0.33788523]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.predict(X_tf_idf_test_multi.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(text):\n",
    "    cl_txt = clean_and_preprocess_text(text)\n",
    "    tfidf_vector_single = vectorizer.transform([cl_txt])\n",
    "    csr_mat = csr_matrix(tfidf_vector_single)\n",
    "    csr_array = csr_mat.toarray()\n",
    "    pred = model_2.predict(csr_array)\n",
    "    rounded_arr = np.round(pred)\n",
    "\n",
    "    # Extract the rounded values \n",
    "    a, b, c = rounded_arr[0]\n",
    "\n",
    "    # Determine the sentiment label based on the rounded values\n",
    "    if a == 1:\n",
    "        return \"Negative\"\n",
    "    elif b == 1:\n",
    "        return \"Positive\"\n",
    "    elif c == 1:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_predict('I love this product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('sent_model.pkl', 'wb') as file:\n",
    "#     pickle.dump(model_2, file)\n",
    "\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
    "#     pickle.dump(vectorizer, file)\n",
    "\n",
    "# file.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': 'Positive'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "tweet = 'I am enjoying the new apple product'\n",
    "\n",
    "url = 'http://localhost:5000/predict'\n",
    "data = {'x': tweet}  # Input data for prediction\n",
    "response = requests.post(url, json=data)\n",
    "result = response.json()\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
