{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Georgia, serif;\">**Twitter Sentiment Analysis** :Understanding Emotions in Tweets about Apple and Google products.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](twits.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Problem**: Using Sentiment Analysis to Improve Apple and Google Product Marketing Strategies \n",
    "\n",
    "The introduction of social media has completely changed how businesses interact with their consumers and the general public in today's connected society. While the digital age offers limitless possibilities for marketing and brand development, it also brings its own set of difficulties. One of these difficulties is the inability of enterprises to precisely gauge public opinion and feelings towards their goods or services.\n",
    "\n",
    "In the age of social media, organizations are acutely aware of the need to harness the wealth of sentiment and emotion data available on these platforms. However, they often struggle to do so effectively, given the unprecedented speed, diversity, and complexity of social media communication. The dynamic nature of the medium, the diverse and contextual language used, the rapid increase of emojis and visual content, the volume of noise, and ethical concerns all contribute to the challenge of gauging public sentiment and emotions. \n",
    "\n",
    "To overcome these challenges, organizations must invest in advanced sentiment analysis tools and technologies, develop cultural and linguistic expertise, and strike a balance between data-driven insights and ethical considerations. By doing so, they can unlock the valuable insights hidden within the social media storm and use them to inform strategic decisions, enhance products and services, and build stronger connections with their audience in this rapidly evolving digital landscape.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def remove_non_utf8(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "with open('data/judge_1377884607_tweet_product_company.csv', 'r', encoding='utf-8') as file:\n",
    "    cleaned_text = remove_non_utf8(file.read())\n",
    "\n",
    "df = pd.read_csv(io.StringIO(cleaned_text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_mapping = {'is_there_an_emotion_directed_at_a_brand_or_product': 'Sentiment'}\n",
    "# Rename the columns using the .rename() method\n",
    "df.rename(columns=column_name_mapping, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emotion_in_tweet_is_directed_at'].fillna('N/A', inplace=True)\n",
    "df['tweet_text'].fillna('N/A', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_brand(phrase):\n",
    "    if 'iPad' in phrase or 'iPhone' in phrase :\n",
    "        return 'Apple'\n",
    "    elif 'Other Apple product or service' in phrase or 'Apple' in phrase:\n",
    "        return 'Apple' \n",
    "    elif 'iPad or iPhone App' in phrase:\n",
    "        return 'Apple'       \n",
    "    elif 'Google' in phrase or 'Other Google product or service' in phrase:\n",
    "        return 'Google'\n",
    "    elif 'Android App' in phrase or 'Android' in phrase:\n",
    "        return 'Android'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "df['brand'] = df['emotion_in_tweet_is_directed_at'].apply(assign_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize,TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def clean_and_preprocess_text(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    # Remove mentions (words starting with '@') and URLs\n",
    "    tokens = [token for token in tokens if not token.startswith('@') and not token.startswith('http')]\n",
    "    # Remove punctuation and numbers using regular expressions\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # # Apply stemming using the Porter Stemmer\n",
    "    # stemmer = PorterStemmer()\n",
    "    # stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    cleaned_text = ' '.join(filtered_tokens) \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet_text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great stuff fri sxsw  marissa mayer  google   tim oreilly  tech books  conferences   matt mullenweg  wordpress '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_and_preprocess_text(df['tweet_text'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df['tweet_text'].map(clean_and_preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>brand</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>RT @mention Front Gate Tickets Present The Mor...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>rt front gate tickets present morning party  s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5099</th>\n",
       "      <td>RT @mention #Want: Micro USB charger for Samsu...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>rt want  micro usb charger samsung android kin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>attending @mention iPad design headaches #sxsw...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>attending ipad design headaches sxsw  link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6525</th>\n",
       "      <td>RT @mention The lines are already forming at A...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>rt lines already forming apples popup store au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>@mention  good morning 2 u!  Enjoy #Sxsw and r...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>good morning  u  enjoy sxsw ride anywhere aust...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "5522  RT @mention Front Gate Tickets Present The Mor...   \n",
       "5099  RT @mention #Want: Micro USB charger for Samsu...   \n",
       "65    attending @mention iPad design headaches #sxsw...   \n",
       "6525  RT @mention The lines are already forming at A...   \n",
       "541   @mention  good morning 2 u!  Enjoy #Sxsw and r...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  Sentiment  brand  \\\n",
       "5522                             N/A        2.0    N/A   \n",
       "5099                             N/A        2.0    N/A   \n",
       "65                              iPad        0.0  Apple   \n",
       "6525                             N/A        2.0    N/A   \n",
       "541                              N/A        2.0    N/A   \n",
       "\n",
       "                                         processed_text  \n",
       "5522  rt front gate tickets present morning party  s...  \n",
       "5099  rt want  micro usb charger samsung android kin...  \n",
       "65          attending ipad design headaches sxsw  link   \n",
       "6525  rt lines already forming apples popup store au...  \n",
       "541   good morning  u  enjoy sxsw ride anywhere aust...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a mapping dictionary\n",
    "sentiment_mapping = {'No emotion toward brand or product': 2.0,\n",
    "                  'Positive emotion': 1.0, \n",
    "                  'Negative emotion': 0.0,\n",
    "                  'I can\\'t tell': 2.0}\n",
    "\n",
    "# Use the .map() method to map values in column 'A' to new values\n",
    "df['Sentiment'] = df['Sentiment'].map(sentiment_mapping)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#creating new df where sentiment is either positive or negative\n",
    "bi_tar = df[(df['Sentiment'] == 0)| (df['Sentiment'] == 1)]\n",
    "\n",
    "X = bi_tar['processed_text']\n",
    "y = bi_tar['Sentiment']\n",
    "\n",
    "X_train_bi, X_test_bi, y_train_bi, y_test_bi = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tar = df.copy()\n",
    "X = multi_tar['processed_text']\n",
    "y = multi_tar['Sentiment']\n",
    "\n",
    "y_dummies = pd.get_dummies(y)\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X, y_dummies, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def csr_Tfid_vect(X_train,X_test):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tf_idf_train = vectorizer.fit_transform(X_train)\n",
    "    tf_idf_test = vectorizer.transform(X_test)\n",
    "\n",
    "    tf_idf_train = csr_matrix(tf_idf_train)\n",
    "    tf_idf_test = csr_matrix(tf_idf_test)\n",
    "\n",
    "    return tf_idf_train,tf_idf_test\n",
    "\n",
    "X_tf_idf_train_bi,X_tf_idf_test_bi = csr_Tfid_vect(X_train_bi,X_test_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "def token_seq(feature):\n",
    "    tokenizer = text.Tokenizer(num_words=20000)\n",
    "    tokenizer.fit_on_texts(list(feature))\n",
    "    list_tokenized = tokenizer.texts_to_sequences(feature)\n",
    "    seq = sequence.pad_sequences(list_tokenized, maxlen=100)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word       Normalized Frequency\n",
      "sxsw              0.0858       \n",
      "mention          0.06442       \n",
      "link             0.03821       \n",
      "rt               0.02743       \n",
      "ipad             0.02671       \n",
      "google            0.022        \n",
      "apple            0.01969       \n",
      "quot             0.01503       \n",
      "iphone           0.01414       \n",
      "store            0.01316       \n"
     ]
    }
   ],
   "source": [
    "from  nltk import FreqDist\n",
    "import string\n",
    "\n",
    "big_sentence = ' '.join(df['tweet_text'])\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "tweets_raw = nltk.regexp_tokenize(big_sentence, pattern)\n",
    "tweets_raw = [word.lower() for word in tweets_raw]\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)\n",
    "tweets_raw_stopped = [word for word in tweets_raw if word not in stopwords_list]\n",
    "tweets_freqdist = FreqDist(tweets_raw_stopped)\n",
    "total_word_count = sum(tweets_freqdist.values())\n",
    "tweets_freqdist_top_10 = tweets_freqdist.most_common(10)\n",
    "print(f'{\"Word\":10} Normalized Frequency')\n",
    "for word in tweets_freqdist_top_10:\n",
    "    normalized_frequency = word[1] / total_word_count\n",
    "    print(f'{word[0]:10} {normalized_frequency:^20.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('rt', 'mention'), 0.02666802617674867),\n",
       " (('sxsw', 'link'), 0.008527835968929014),\n",
       " (('link', 'sxsw'), 0.007656513598190615),\n",
       " (('sxsw', 'rt'), 0.006275374946701025),\n",
       " (('mention', 'mention'), 0.005728481118258838),\n",
       " (('mention', 'sxsw'), 0.005478207671344618),\n",
       " (('apple', 'store'), 0.005348436254426132),\n",
       " (('sxsw', 'mention'), 0.004755195491370201),\n",
       " (('link', 'rt'), 0.004718117943679205),\n",
       " (('mention', 'google'), 0.004356611853691997),\n",
       " (('social', 'network'), 0.0040970690198550265),\n",
       " (('new', 'social'), 0.003781909864481563),\n",
       " (('mention', 'rt'), 0.0031886691014256317),\n",
       " (('network', 'called'), 0.003021820136816151),\n",
       " (('store', 'sxsw'), 0.003021820136816151)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "tweets_finder = BigramCollocationFinder.from_words(tweets_raw_stopped)\n",
    "tweets_scored = tweets_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "tweets_scored[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SXSW is best known for its conference and festivals that celebrate the convergence of tech, film, music, education, and culture.\n",
    "RT is the first Russian 24/7 English-language news channel which brings the Russian view on global news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(724350, 978660)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "\n",
    "data = df['processed_text'].map(word_tokenize)\n",
    "model = Word2Vec(data, window=5, min_count=1, workers=4)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('grip', 0.6849561333656311),\n",
       " ('rt', 0.683631420135498),\n",
       " ('theinternet', 0.6761109232902527),\n",
       " ('steam', 0.6695597767829895),\n",
       " ('pun', 0.6666953563690186),\n",
       " ('edapps', 0.6555663347244263),\n",
       " ('brk', 0.6528748273849487),\n",
       " ('festivities', 0.6442430019378662),\n",
       " ('fallen', 0.6419836282730103),\n",
       " ('franco', 0.6408178210258484)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv = model.wv\n",
    "wv.most_similar('sxsw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ringing', 0.4128453731536865),\n",
       " ('impedimenta', 0.3832816481590271),\n",
       " ('comedy', 0.30024275183677673),\n",
       " ('precommerce', 0.29874300956726074),\n",
       " ('rows', 0.273887038230896),\n",
       " ('whowillrise', 0.26472175121307373),\n",
       " ('amismarternow', 0.25267156958580017),\n",
       " ('arrive', 0.24874566495418549),\n",
       " ('evacuation', 0.2394924759864807),\n",
       " ('linney', 0.23355400562286377)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(negative='sxsw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Model**: Sentiment is either positive(1)or negative(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   12.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   10.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.8593924133455613),\n",
       " ('Support Vector Machine', 0.8568296515587877),\n",
       " ('Logistic Regression', 0.8443789251256308)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([('Logistic Regression', LogisticRegression())])\n",
    "\n",
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]\n",
    "\n",
    "scores = [(name, cross_val_score(model,X_tf_idf_train_bi, y_train_bi, cv=2).mean()) for name, model, in models]\n",
    "scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t =  token_seq(X_train_bi) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                6464      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8577 (33.50 KB)\n",
      "Trainable params: 8577 (33.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(Dense(units=64, input_shape=(100,)))\n",
    "model_1.add(Dropout(0.5))\n",
    "\n",
    "model_1.add(Dense(32, activation='relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_1.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "77/77 [==============================] - 7s 24ms/step - loss: 173.3126 - accuracy: 0.6679 - val_loss: 64.5030 - val_accuracy: 0.8394\n",
      "Epoch 2/15\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 98.6385 - accuracy: 0.6984 - val_loss: 35.0356 - val_accuracy: 0.8248\n",
      "Epoch 3/15\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 61.4895 - accuracy: 0.6927 - val_loss: 18.5338 - val_accuracy: 0.7883\n",
      "Epoch 4/15\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 41.0096 - accuracy: 0.7033 - val_loss: 9.0170 - val_accuracy: 0.7263\n",
      "Epoch 5/15\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 24.9330 - accuracy: 0.7057 - val_loss: 3.3315 - val_accuracy: 0.8285\n",
      "Epoch 6/15\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 15.7804 - accuracy: 0.7289 - val_loss: 1.1552 - val_accuracy: 0.8102\n",
      "Epoch 7/15\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 9.2919 - accuracy: 0.7542 - val_loss: 0.8748 - val_accuracy: 0.8358\n",
      "Epoch 8/15\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 6.8646 - accuracy: 0.7810 - val_loss: 0.7126 - val_accuracy: 0.8358\n",
      "Epoch 9/15\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 5.7067 - accuracy: 0.7786 - val_loss: 0.6225 - val_accuracy: 0.8358\n",
      "Epoch 10/15\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 4.5745 - accuracy: 0.7794 - val_loss: 0.6436 - val_accuracy: 0.7299\n",
      "Epoch 11/15\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 2.5680 - accuracy: 0.8083 - val_loss: 0.5599 - val_accuracy: 0.7956\n",
      "Epoch 12/15\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.3156 - accuracy: 0.8006 - val_loss: 0.5509 - val_accuracy: 0.8175\n",
      "Epoch 13/15\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 2.1123 - accuracy: 0.8116 - val_loss: 0.4908 - val_accuracy: 0.8358\n",
      "Epoch 14/15\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 2.4149 - accuracy: 0.8014 - val_loss: 0.4925 - val_accuracy: 0.8358\n",
      "Epoch 15/15\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 1.7764 - accuracy: 0.8144 - val_loss: 0.4866 - val_accuracy: 0.8358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1de87535630>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(X_t, y_train_bi, epochs=15, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterated Model**: Sentiment is either positive(1),negative(0),No emotion toward brand or product(2) or Not clear(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t_multi = token_seq(X_train_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "197/197 [==============================] - 6s 14ms/step - loss: 272.4950 - accuracy: 0.4511 - val_loss: 101.4260 - val_accuracy: 0.5301\n",
      "Epoch 2/15\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 106.0558 - accuracy: 0.4575 - val_loss: 49.4055 - val_accuracy: 0.5229\n",
      "Epoch 3/15\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 40.8878 - accuracy: 0.4705 - val_loss: 23.7432 - val_accuracy: 0.5487\n",
      "Epoch 4/15\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 17.4272 - accuracy: 0.4850 - val_loss: 11.3415 - val_accuracy: 0.5458\n",
      "Epoch 5/15\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.4152 - accuracy: 0.4876 - val_loss: 6.2712 - val_accuracy: 0.5057\n",
      "Epoch 6/15\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.8393 - accuracy: 0.5126 - val_loss: 4.6779 - val_accuracy: 0.5129\n",
      "Epoch 7/15\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 3.0378 - accuracy: 0.5247 - val_loss: 3.5697 - val_accuracy: 0.5473\n",
      "Epoch 8/15\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 2.5015 - accuracy: 0.5284 - val_loss: 2.7043 - val_accuracy: 0.5401\n",
      "Epoch 9/15\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.1542 - accuracy: 0.5384 - val_loss: 2.9788 - val_accuracy: 0.5645\n",
      "Epoch 10/15\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.6602 - accuracy: 0.5389 - val_loss: 2.3991 - val_accuracy: 0.5788\n",
      "Epoch 11/15\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.6082 - accuracy: 0.5505 - val_loss: 2.5697 - val_accuracy: 0.5788\n",
      "Epoch 12/15\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 1.4109 - accuracy: 0.5475 - val_loss: 1.8546 - val_accuracy: 0.5229\n",
      "Epoch 13/15\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.2931 - accuracy: 0.5577 - val_loss: 1.9291 - val_accuracy: 0.5573\n",
      "Epoch 14/15\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2181 - accuracy: 0.5615 - val_loss: 1.8090 - val_accuracy: 0.5272\n",
      "Epoch 15/15\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.1899 - accuracy: 0.5602 - val_loss: 1.7308 - val_accuracy: 0.5573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1de87386620>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "\n",
    "model_2.add(Dense(64, activation='relu', input_shape=(100,)))\n",
    "model_2.add(Dropout(0.5))\n",
    "\n",
    "model_2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model_2.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_2.fit(X_t_multi, y_train_multi, epochs=15, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9174 unique tokens in the dataset.\n"
     ]
    }
   ],
   "source": [
    "total_vocabulary = set(word for headline in data for word in headline)\n",
    "print('There are {} unique tokens in the dataset.'.format(len(total_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "glove = {}\n",
    "with open('glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [d for d in df['Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   20.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   21.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.6397201845380973)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "\n",
    "models = [('Random Forest', rf)]\n",
    "\n",
    "scores = [(name, cross_val_score(model, data,c, cv=2).mean()) for name, model, in models]\n",
    "scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   48.5s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Word2Vec Vectorizer&#x27;,\n",
       "                 &lt;__main__.W2vVectorizer object at 0x000001DE88ADF520&gt;),\n",
       "                (&#x27;Random Forest&#x27;, RandomForestClassifier(verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Word2Vec Vectorizer&#x27;,\n",
       "                 &lt;__main__.W2vVectorizer object at 0x000001DE88ADF520&gt;),\n",
       "                (&#x27;Random Forest&#x27;, RandomForestClassifier(verbose=True))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">W2vVectorizer</label><div class=\"sk-toggleable__content\"><pre>&lt;__main__.W2vVectorizer object at 0x000001DE88ADF520&gt;</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(verbose=True)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Word2Vec Vectorizer',\n",
       "                 <__main__.W2vVectorizer object at 0x000001DE88ADF520>),\n",
       "                ('Random Forest', RandomForestClassifier(verbose=True))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(data,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_new_column(value):\n",
    "    if value == 0.0:\n",
    "        return 'negative'\n",
    "    elif value == 1.0:\n",
    "        return 'positive'\n",
    "    elif value == 2.0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'unknown'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(filepath,tweet_column,dir_at,model):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        cleaned_text = remove_non_utf8(file.read())\n",
    "    df = pd.read_csv(io.StringIO(cleaned_text))\n",
    "\n",
    "    df[tweet_column].fillna('N/A', inplace=True)\n",
    "    df['processed_text'] = df[tweet_column].map(clean_and_preprocess_text)\n",
    "\n",
    "    df[dir_at].fillna('N/A', inplace=True)\n",
    "    df['brand'] = df[dir_at].apply(assign_brand)\n",
    "\n",
    "    data = df['processed_text'].map(word_tokenize)\n",
    "    pred = model.predict(data)\n",
    "\n",
    "    df['pred'] = pred\n",
    "    df['prediction'] = df['pred'].apply(map_to_new_column)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>brand</th>\n",
       "      <th>pred</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5870</th>\n",
       "      <td>RT @mention If you were able to afford to atte...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>rt able afford attend sxsw buy ipad today  con...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6868</th>\n",
       "      <td>Why do people at #sxswi insist on sharing foot...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>people sxswi insist sharing footage japanese t...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>Platform for Concertgoers Launches Android App...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>platform concertgoers launches android app tim...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6654</th>\n",
       "      <td>RT @mention We just launched our iPad app at #...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>rt launched ipad app sxsw  get details  first ...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>1.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7113</th>\n",
       "      <td>Awesome iPhone cases for @mention here at #sxsw!</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>awesome iphone cases sxsw</td>\n",
       "      <td>Apple</td>\n",
       "      <td>1.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "5870  RT @mention If you were able to afford to atte...   \n",
       "6868  Why do people at #sxswi insist on sharing foot...   \n",
       "3015  Platform for Concertgoers Launches Android App...   \n",
       "6654  RT @mention We just launched our iPad app at #...   \n",
       "7113   Awesome iPhone cases for @mention here at #sxsw!   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "5870                             N/A   \n",
       "6868                             N/A   \n",
       "3015                             N/A   \n",
       "6654              iPad or iPhone App   \n",
       "7113                          iPhone   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "5870                 No emotion toward brand or product   \n",
       "6868                 No emotion toward brand or product   \n",
       "3015                 No emotion toward brand or product   \n",
       "6654                                   Positive emotion   \n",
       "7113                                   Positive emotion   \n",
       "\n",
       "                                         processed_text  brand  pred  \\\n",
       "5870  rt able afford attend sxsw buy ipad today  con...    N/A   2.0   \n",
       "6868  people sxswi insist sharing footage japanese t...    N/A   2.0   \n",
       "3015  platform concertgoers launches android app tim...    N/A   2.0   \n",
       "6654  rt launched ipad app sxsw  get details  first ...  Apple   1.0   \n",
       "7113                         awesome iphone cases sxsw   Apple   1.0   \n",
       "\n",
       "     prediction  \n",
       "5870    neutral  \n",
       "6868    neutral  \n",
       "3015    neutral  \n",
       "6654   positive  \n",
       "7113   positive  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'data/judge_1377884607_tweet_product_company.csv'\n",
    "tweet_column = 'tweet_text'\n",
    "dir_at = 'emotion_in_tweet_is_directed_at'\n",
    "model = rf\n",
    "\n",
    "pred_df = sentiment_predict(filepath,tweet_column,dir_at,model)\n",
    "pred_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iPad                              119\n",
       "iPhone                             98\n",
       "Apple                              91\n",
       "iPad or iPhone App                 60\n",
       "Other Apple product or service      2\n",
       "Name: emotion_in_tweet_is_directed_at, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#positive apple tweets\n",
    "apple_pos = pred_df[(pred_df['brand'] == 'Apple') & (pred_df['prediction'] == 'positive')]\n",
    "#negative apple tweets\n",
    "apple_neg = pred_df[(pred_df['brand'] == 'Apple') & (pred_df['prediction'] == 'negative')]\n",
    "apple_neg.emotion_in_tweet_is_directed_at.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Google                             63\n",
       "Other Google product or service    46\n",
       "Name: emotion_in_tweet_is_directed_at, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#positive Google tweets\n",
    "Google_pos = pred_df[(pred_df['brand'] == 'Google') & (pred_df['prediction'] == 'positive')]\n",
    "#negative google tweets\n",
    "Google_neg = pred_df[(pred_df['brand'] == 'Google') & (pred_df['prediction'] == 'negative')]\n",
    "Google_neg.emotion_in_tweet_is_directed_at.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Android        9\n",
       "Android App    7\n",
       "Name: emotion_in_tweet_is_directed_at, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#positive Android tweets\n",
    "Android_pos = pred_df[(pred_df['brand'] == 'Android') & (pred_df['prediction'] == 'positive')]\n",
    "#negative google tweets\n",
    "Android_neg = pred_df[(pred_df['brand'] == 'Android') & (pred_df['prediction'] == 'negative')]\n",
    "Android_neg.emotion_in_tweet_is_directed_at.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('model_rf.pkl', 'wb') as file:\n",
    "#     pickle.dump(rf, file)\n",
    "\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
