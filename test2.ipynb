{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Georgia, serif;\">**Twitter Sentiment Analysis** :Understanding Emotions in Tweets about Apple and Google products.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](twits.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Problem**: Using Sentiment Analysis to Improve Apple and Google Product Marketing Strategies \n",
    "\n",
    "The introduction of social media has completely changed how businesses interact with their consumers and the general public in today's connected society. While the digital age offers limitless possibilities for marketing and brand development, it also brings its own set of difficulties. One of these difficulties is the inability of enterprises to precisely gauge public opinion and feelings towards their goods or services.\n",
    "\n",
    "In the age of social media, organizations are acutely aware of the need to harness the wealth of sentiment and emotion data available on these platforms. However, they often struggle to do so effectively, given the unprecedented speed, diversity, and complexity of social media communication. The dynamic nature of the medium, the diverse and contextual language used, the rapid increase of emojis and visual content, the volume of noise, and ethical concerns all contribute to the challenge of gauging public sentiment and emotions. \n",
    "\n",
    "To overcome these challenges, organizations must invest in advanced sentiment analysis tools and technologies, develop cultural and linguistic expertise, and strike a balance between data-driven insights and ethical considerations. By doing so, they can unlock the valuable insights hidden within the social media storm and use them to inform strategic decisions, enhance products and services, and build stronger connections with their audience in this rapidly evolving digital landscape.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def remove_non_utf8(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "with open('data/judge_1377884607_tweet_product_company.csv', 'r', encoding='utf-8') as file:\n",
    "    cleaned_text = remove_non_utf8(file.read())\n",
    "\n",
    "df = pd.read_csv(io.StringIO(cleaned_text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_mapping = {'is_there_an_emotion_directed_at_a_brand_or_product': 'Sentiment'}\n",
    "# Rename the columns using the .rename() method\n",
    "df.rename(columns=column_name_mapping, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emotion_in_tweet_is_directed_at'].fillna('N/A', inplace=True)\n",
    "df['tweet_text'].fillna('N/A', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_brand(phrase):\n",
    "    if 'iPad' in phrase or 'iPhone' in phrase :\n",
    "        return 'Apple'\n",
    "    elif 'Other Apple product or service' in phrase or 'Apple' in phrase:\n",
    "        return 'Apple' \n",
    "    elif 'iPad or iPhone App' in phrase:\n",
    "        return 'Apple'       \n",
    "    elif 'Google' in phrase or 'Other Google product or service' in phrase:\n",
    "        return 'Google'\n",
    "    elif 'Android App' in phrase or 'Android' in phrase:\n",
    "        return 'Android'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "df['brand'] = df['emotion_in_tweet_is_directed_at'].apply(assign_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize,TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def clean_and_preprocess_text(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    # Remove mentions (words starting with '@') and URLs\n",
    "    tokens = [token for token in tokens if not token.startswith('@') and not token.startswith('http')]\n",
    "    # Remove punctuation and numbers using regular expressions\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # # Apply stemming using the Porter Stemmer\n",
    "    # stemmer = PorterStemmer()\n",
    "    # stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    cleaned_text = ' '.join(filtered_tokens) \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet_text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great stuff fri sxsw  marissa mayer  google   tim oreilly  tech books  conferences   matt mullenweg  wordpress '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_and_preprocess_text(df['tweet_text'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df['tweet_text'].map(clean_and_preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>brand</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8308</th>\n",
       "      <td>wish i could go @mention Head over to {link} b...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>wish could go head  link  pm cst today win vip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>Headed to Designing iPad Interfaces - New Navi...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>headed designing ipad interfaces  new navigati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124</th>\n",
       "      <td>&amp;quot;google has too many products, and needs ...</td>\n",
       "      <td>Google</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Google</td>\n",
       "      <td>google many products  needs condense  sxsw  f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8177</th>\n",
       "      <td>omg @mention It's not a rumor: Apple is openin...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>omg rumor  apple opening temporary store downt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8102</th>\n",
       "      <td>Line for the iPad 2, at the Apple Store. No de...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>line ipad   apple store  demo units pm  sxsw f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "8308  wish i could go @mention Head over to {link} b...   \n",
       "424   Headed to Designing iPad Interfaces - New Navi...   \n",
       "3124  &quot;google has too many products, and needs ...   \n",
       "8177  omg @mention It's not a rumor: Apple is openin...   \n",
       "8102  Line for the iPad 2, at the Apple Store. No de...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  Sentiment   brand  \\\n",
       "8308                             N/A        2.0     N/A   \n",
       "424                              N/A        2.0     N/A   \n",
       "3124                          Google        0.0  Google   \n",
       "8177                           Apple        1.0   Apple   \n",
       "8102                            iPad        1.0   Apple   \n",
       "\n",
       "                                         processed_text  \n",
       "8308  wish could go head  link  pm cst today win vip...  \n",
       "424   headed designing ipad interfaces  new navigati...  \n",
       "3124   google many products  needs condense  sxsw  f...  \n",
       "8177  omg rumor  apple opening temporary store downt...  \n",
       "8102  line ipad   apple store  demo units pm  sxsw f...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a mapping dictionary\n",
    "sentiment_mapping = {'No emotion toward brand or product': 2.0,\n",
    "                  'Positive emotion': 1.0, \n",
    "                  'Negative emotion': 0.0,\n",
    "                  'I can\\'t tell': 2.0}\n",
    "\n",
    "# Use the .map() method to map values in column 'A' to new values\n",
    "df['Sentiment'] = df['Sentiment'].map(sentiment_mapping)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#creating new df where sentiment is either positive or negative\n",
    "bi_tar = df[(df['Sentiment'] == 0)| (df['Sentiment'] == 1)]\n",
    "\n",
    "X = bi_tar['processed_text']\n",
    "y = bi_tar['Sentiment']\n",
    "\n",
    "X_train_bi, X_test_bi, y_train_bi, y_test_bi = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tar = df.copy()\n",
    "X = multi_tar['processed_text']\n",
    "y = multi_tar['Sentiment']\n",
    "\n",
    "y_dummies = pd.get_dummies(y)\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X, y_dummies, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def csr_Tfid_vect(X_train,X_test):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tf_idf_train = vectorizer.fit_transform(X_train)\n",
    "    tf_idf_test = vectorizer.transform(X_test)\n",
    "\n",
    "    tf_idf_train = csr_matrix(tf_idf_train)\n",
    "    tf_idf_test = csr_matrix(tf_idf_test)\n",
    "\n",
    "    return tf_idf_train,tf_idf_test\n",
    "\n",
    "X_tf_idf_train_bi,X_tf_idf_test_bi = csr_Tfid_vect(X_train_bi,X_test_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "def token_seq(feature):\n",
    "    tokenizer = text.Tokenizer(num_words=20000)\n",
    "    tokenizer.fit_on_texts(list(feature))\n",
    "    list_tokenized = tokenizer.texts_to_sequences(feature)\n",
    "    seq = sequence.pad_sequences(list_tokenized, maxlen=100)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word       Normalized Frequency\n",
      "sxsw              0.0858       \n",
      "mention          0.06442       \n",
      "link             0.03821       \n",
      "rt               0.02743       \n",
      "ipad             0.02671       \n",
      "google            0.022        \n",
      "apple            0.01969       \n",
      "quot             0.01503       \n",
      "iphone           0.01414       \n",
      "store            0.01316       \n"
     ]
    }
   ],
   "source": [
    "from  nltk import FreqDist\n",
    "import string\n",
    "\n",
    "big_sentence = ' '.join(df['tweet_text'])\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "tweets_raw = nltk.regexp_tokenize(big_sentence, pattern)\n",
    "tweets_raw = [word.lower() for word in tweets_raw]\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)\n",
    "tweets_raw_stopped = [word for word in tweets_raw if word not in stopwords_list]\n",
    "tweets_freqdist = FreqDist(tweets_raw_stopped)\n",
    "total_word_count = sum(tweets_freqdist.values())\n",
    "tweets_freqdist_top_10 = tweets_freqdist.most_common(10)\n",
    "print(f'{\"Word\":10} Normalized Frequency')\n",
    "for word in tweets_freqdist_top_10:\n",
    "    normalized_frequency = word[1] / total_word_count\n",
    "    print(f'{word[0]:10} {normalized_frequency:^20.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('rt', 'mention'), 0.02666802617674867),\n",
       " (('sxsw', 'link'), 0.008527835968929014),\n",
       " (('link', 'sxsw'), 0.007656513598190615),\n",
       " (('sxsw', 'rt'), 0.006275374946701025),\n",
       " (('mention', 'mention'), 0.005728481118258838),\n",
       " (('mention', 'sxsw'), 0.005478207671344618),\n",
       " (('apple', 'store'), 0.005348436254426132),\n",
       " (('sxsw', 'mention'), 0.004755195491370201),\n",
       " (('link', 'rt'), 0.004718117943679205),\n",
       " (('mention', 'google'), 0.004356611853691997),\n",
       " (('social', 'network'), 0.0040970690198550265),\n",
       " (('new', 'social'), 0.003781909864481563),\n",
       " (('mention', 'rt'), 0.0031886691014256317),\n",
       " (('network', 'called'), 0.003021820136816151),\n",
       " (('store', 'sxsw'), 0.003021820136816151)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "tweets_finder = BigramCollocationFinder.from_words(tweets_raw_stopped)\n",
    "tweets_scored = tweets_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "tweets_scored[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SXSW is best known for its conference and festivals that celebrate the convergence of tech, film, music, education, and culture.\n",
    "RT is the first Russian 24/7 English-language news channel which brings the Russian view on global news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(724523, 978660)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "\n",
    "data = df['processed_text'].map(word_tokenize)\n",
    "model = Word2Vec(data, window=5, min_count=1, workers=4)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('notsosecret', 0.6787000894546509),\n",
       " ('samsungsxsw', 0.6743574142456055),\n",
       " ('craps', 0.6580419540405273),\n",
       " ('socmedia', 0.6464982628822327),\n",
       " ('ireport', 0.6461699604988098),\n",
       " ('south', 0.6448580026626587),\n",
       " ('cabs', 0.6438364386558533),\n",
       " ('frickin', 0.6380432844161987),\n",
       " ('greatergood', 0.6293945908546448),\n",
       " ('ismparty', 0.627713143825531)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv = model.wv\n",
    "wv.most_similar('sxsw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sqchat', 0.37491530179977417),\n",
       " ('delivery', 0.3722917139530182),\n",
       " ('ringing', 0.3664971590042114),\n",
       " ('newapplestoreaustin', 0.3149334192276001),\n",
       " ('warmer', 0.3019205331802368),\n",
       " ('forever', 0.27612364292144775),\n",
       " ('mocking', 0.26970696449279785),\n",
       " ('aug', 0.25624948740005493),\n",
       " ('petricone', 0.25289714336395264),\n",
       " ('questioner', 0.24092993140220642)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(negative='sxsw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Model**: Sentiment is either positive(1)or negative(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   12.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   10.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.8630540762312359),\n",
       " ('Support Vector Machine', 0.8568296515587877),\n",
       " ('Logistic Regression', 0.8443789251256308)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([('Logistic Regression', LogisticRegression())])\n",
    "\n",
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]\n",
    "\n",
    "scores = [(name, cross_val_score(model,X_tf_idf_train_bi, y_train_bi, cv=2).mean()) for name, model, in models]\n",
    "scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t =  token_seq(X_train_bi) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                6464      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8577 (33.50 KB)\n",
      "Trainable params: 8577 (33.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(Dense(units=64, input_shape=(100,)))\n",
    "model_1.add(Dropout(0.5))\n",
    "\n",
    "model_1.add(Dense(32, activation='relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_1.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "77/77 [==============================] - 7s 25ms/step - loss: 162.7058 - accuracy: 0.6736 - val_loss: 73.1151 - val_accuracy: 0.8248\n",
      "Epoch 2/15\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 92.1471 - accuracy: 0.6927 - val_loss: 39.7439 - val_accuracy: 0.8212\n",
      "Epoch 3/15\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 53.8014 - accuracy: 0.6931 - val_loss: 10.7263 - val_accuracy: 0.7482\n",
      "Epoch 4/15\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 33.2763 - accuracy: 0.6911 - val_loss: 4.9072 - val_accuracy: 0.7664\n",
      "Epoch 5/15\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 18.7806 - accuracy: 0.7293 - val_loss: 1.8408 - val_accuracy: 0.8175\n",
      "Epoch 6/15\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 11.2163 - accuracy: 0.7428 - val_loss: 0.9618 - val_accuracy: 0.6569\n",
      "Epoch 7/15\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 6.8843 - accuracy: 0.7676 - val_loss: 0.7659 - val_accuracy: 0.8321\n",
      "Epoch 8/15\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 5.7132 - accuracy: 0.7859 - val_loss: 0.6499 - val_accuracy: 0.8321\n",
      "Epoch 9/15\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 4.1519 - accuracy: 0.7965 - val_loss: 0.5538 - val_accuracy: 0.8358\n",
      "Epoch 10/15\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 2.8436 - accuracy: 0.8071 - val_loss: 0.5862 - val_accuracy: 0.8139\n",
      "Epoch 11/15\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 2.9420 - accuracy: 0.8201 - val_loss: 0.5691 - val_accuracy: 0.8321\n",
      "Epoch 12/15\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7668 - accuracy: 0.8132 - val_loss: 0.5132 - val_accuracy: 0.8358\n",
      "Epoch 13/15\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 1.8150 - accuracy: 0.8201 - val_loss: 0.5115 - val_accuracy: 0.8358\n",
      "Epoch 14/15\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.1262 - accuracy: 0.8246 - val_loss: 0.5059 - val_accuracy: 0.8358\n",
      "Epoch 15/15\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 2.0248 - accuracy: 0.8234 - val_loss: 0.5038 - val_accuracy: 0.8358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d611ae12d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(X_t, y_train_bi, epochs=15, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterated Model**: Sentiment is either positive(1),negative(0),No emotion toward brand or product(2) or Not clear(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t_multi = token_seq(X_train_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "197/197 [==============================] - 5s 11ms/step - loss: 269.9518 - accuracy: 0.4439 - val_loss: 121.5970 - val_accuracy: 0.5573\n",
      "Epoch 2/15\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 100.9804 - accuracy: 0.4665 - val_loss: 50.4115 - val_accuracy: 0.5315\n",
      "Epoch 3/15\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 42.6108 - accuracy: 0.4689 - val_loss: 24.6269 - val_accuracy: 0.5372\n",
      "Epoch 4/15\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 19.2499 - accuracy: 0.4825 - val_loss: 12.2921 - val_accuracy: 0.5659\n",
      "Epoch 5/15\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 8.8626 - accuracy: 0.4928 - val_loss: 6.7513 - val_accuracy: 0.5616\n",
      "Epoch 6/15\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 5.1705 - accuracy: 0.4995 - val_loss: 4.0073 - val_accuracy: 0.5473\n",
      "Epoch 7/15\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 3.0224 - accuracy: 0.5266 - val_loss: 2.7932 - val_accuracy: 0.5043\n",
      "Epoch 8/15\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 2.2669 - accuracy: 0.5322 - val_loss: 2.1770 - val_accuracy: 0.5487\n",
      "Epoch 9/15\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.7226 - accuracy: 0.5545 - val_loss: 1.8776 - val_accuracy: 0.5788\n",
      "Epoch 10/15\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 1.7716 - accuracy: 0.5511 - val_loss: 1.7223 - val_accuracy: 0.5616\n",
      "Epoch 11/15\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.5395 - accuracy: 0.5554 - val_loss: 1.4754 - val_accuracy: 0.5215\n",
      "Epoch 12/15\n",
      "197/197 [==============================] - 1s 8ms/step - loss: 1.2973 - accuracy: 0.5645 - val_loss: 1.4872 - val_accuracy: 0.5344\n",
      "Epoch 13/15\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.2963 - accuracy: 0.5615 - val_loss: 1.3875 - val_accuracy: 0.5501\n",
      "Epoch 14/15\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 1.3412 - accuracy: 0.5626 - val_loss: 1.3197 - val_accuracy: 0.5358\n",
      "Epoch 15/15\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 1.1812 - accuracy: 0.5644 - val_loss: 1.0714 - val_accuracy: 0.5759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d61191b040>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "\n",
    "model_2.add(Dense(64, activation='relu', input_shape=(100,)))\n",
    "model_2.add(Dropout(0.5))\n",
    "\n",
    "model_2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model_2.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_2.fit(X_t_multi, y_train_multi, epochs=15, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9174 unique tokens in the dataset.\n"
     ]
    }
   ],
   "source": [
    "total_vocabulary = set(word for headline in data for word in headline)\n",
    "print('There are {} unique tokens in the dataset.'.format(len(total_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "glove = {}\n",
    "with open('glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [d for d in df['Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   22.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   22.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.642701835914244)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "\n",
    "models = [('Random Forest', rf)]\n",
    "\n",
    "scores = [(name, cross_val_score(model, data,c, cv=2).mean()) for name, model, in models]\n",
    "scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   47.1s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Word2Vec Vectorizer&#x27;,\n",
       "                 &lt;__main__.W2vVectorizer object at 0x000001D612E335B0&gt;),\n",
       "                (&#x27;Random Forest&#x27;, RandomForestClassifier(verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Word2Vec Vectorizer&#x27;,\n",
       "                 &lt;__main__.W2vVectorizer object at 0x000001D612E335B0&gt;),\n",
       "                (&#x27;Random Forest&#x27;, RandomForestClassifier(verbose=True))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">W2vVectorizer</label><div class=\"sk-toggleable__content\"><pre>&lt;__main__.W2vVectorizer object at 0x000001D612E335B0&gt;</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(verbose=True)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Word2Vec Vectorizer',\n",
       "                 <__main__.W2vVectorizer object at 0x000001D612E335B0>),\n",
       "                ('Random Forest', RandomForestClassifier(verbose=True))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(data,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_new_column(value):\n",
    "    if value == 0.0:\n",
    "        return 'negative'\n",
    "    elif value == 1.0:\n",
    "        return 'positive'\n",
    "    elif value == 2.0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'unknown'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(filepath,tweet_column,dir_at,model):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        cleaned_text = remove_non_utf8(file.read())\n",
    "    df = pd.read_csv(io.StringIO(cleaned_text))\n",
    "\n",
    "    df[tweet_column].fillna('N/A', inplace=True)\n",
    "    df['processed_text'] = df[tweet_column].map(clean_and_preprocess_text)\n",
    "\n",
    "    df[dir_at].fillna('N/A', inplace=True)\n",
    "    df['brand'] = df[dir_at].apply(assign_brand)\n",
    "\n",
    "    data = df['processed_text'].map(word_tokenize)\n",
    "    pred = model.predict(data)\n",
    "\n",
    "    df['pred'] = pred\n",
    "    df['prediction'] = df['pred'].apply(map_to_new_column)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>brand</th>\n",
       "      <th>pred</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7951</th>\n",
       "      <td>Free Smart Recorder for this weekend only {lin...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>free smart recorder weekend  link  via iphone ...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>1.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7162</th>\n",
       "      <td>Does the pop up Apple store in Austin still ha...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>pop apple store austin still ipad  stock  lazy...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5199</th>\n",
       "      <td>RT @mention Apple is opening a store at #SXSW....</td>\n",
       "      <td>N/A</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>rt apple opening store sxsw   link</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RT @LaurieShook: I'm looking forward to the #S...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>rt  im looking forward smcdallas pre sxsw part...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>1.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5670</th>\n",
       "      <td>RT @mention Google to Launch Major New Social ...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>rt google launch major new social network call...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "7951  Free Smart Recorder for this weekend only {lin...   \n",
       "7162  Does the pop up Apple store in Austin still ha...   \n",
       "5199  RT @mention Apple is opening a store at #SXSW....   \n",
       "25    RT @LaurieShook: I'm looking forward to the #S...   \n",
       "5670  RT @mention Google to Launch Major New Social ...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "7951                          iPhone   \n",
       "7162                             N/A   \n",
       "5199                             N/A   \n",
       "25                              iPad   \n",
       "5670                             N/A   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "7951                                   Positive emotion   \n",
       "7162                 No emotion toward brand or product   \n",
       "5199                 No emotion toward brand or product   \n",
       "25                                     Positive emotion   \n",
       "5670                                   Positive emotion   \n",
       "\n",
       "                                         processed_text  brand  pred  \\\n",
       "7951  free smart recorder weekend  link  via iphone ...  Apple   1.0   \n",
       "7162  pop apple store austin still ipad  stock  lazy...    N/A   2.0   \n",
       "5199                rt apple opening store sxsw   link     N/A   2.0   \n",
       "25    rt  im looking forward smcdallas pre sxsw part...  Apple   1.0   \n",
       "5670  rt google launch major new social network call...    N/A   2.0   \n",
       "\n",
       "     prediction  \n",
       "7951   positive  \n",
       "7162    neutral  \n",
       "5199    neutral  \n",
       "25     positive  \n",
       "5670    neutral  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'data/judge_1377884607_tweet_product_company.csv'\n",
    "tweet_column = 'tweet_text'\n",
    "dir_at = 'emotion_in_tweet_is_directed_at'\n",
    "model = rf\n",
    "\n",
    "pred_df = sentiment_predict(filepath,tweet_column,dir_at,model)\n",
    "pred_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iPad                              116\n",
       "iPhone                             98\n",
       "Apple                              91\n",
       "iPad or iPhone App                 60\n",
       "Other Apple product or service      2\n",
       "Name: emotion_in_tweet_is_directed_at, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#positive apple tweets\n",
    "apple_pos = pred_df[(pred_df['brand'] == 'Apple') & (pred_df['prediction'] == 'positive')]\n",
    "#negative apple tweets\n",
    "apple_neg = pred_df[(pred_df['brand'] == 'Apple') & (pred_df['prediction'] == 'negative')]\n",
    "apple_neg.emotion_in_tweet_is_directed_at.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Google                             61\n",
       "Other Google product or service    46\n",
       "Name: emotion_in_tweet_is_directed_at, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#positive Google tweets\n",
    "Google_pos = pred_df[(pred_df['brand'] == 'Google') & (pred_df['prediction'] == 'positive')]\n",
    "#negative google tweets\n",
    "Google_neg = pred_df[(pred_df['brand'] == 'Google') & (pred_df['prediction'] == 'negative')]\n",
    "Google_neg.emotion_in_tweet_is_directed_at.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Android        7\n",
       "Android App    7\n",
       "Name: emotion_in_tweet_is_directed_at, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#positive Android tweets\n",
    "Android_pos = pred_df[(pred_df['brand'] == 'Android') & (pred_df['prediction'] == 'positive')]\n",
    "#negative google tweets\n",
    "Android_neg = pred_df[(pred_df['brand'] == 'Android') & (pred_df['prediction'] == 'negative')]\n",
    "Android_neg.emotion_in_tweet_is_directed_at.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('model_rf.pkl', 'wb') as file:\n",
    "#     pickle.dump(rf, file)\n",
    "\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
